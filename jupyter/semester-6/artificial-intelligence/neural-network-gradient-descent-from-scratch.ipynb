{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 XOR Neural Network with Gradient Descent from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, layer_neurons, inputs=False, input_bias=False):\n",
    "        self.layer_neurons = layer_neurons\n",
    "        self.input_bias = input_bias\n",
    "        self.inputs = inputs + input_bias  # If bias input would increase by 1\n",
    "        self.delta_weights = 0\n",
    "        if inputs:\n",
    "            self.weights = np.array([[rand.uniform(-1,1) for _ in range(self.layer_neurons)]  for _ in range(self.inputs)])   # (inputs, layer_neurons) shape of each layer's weights\n",
    "    \n",
    "    # This method initializes delta_weights\n",
    "    def initialize_delta_weights(self):\n",
    "        self.delta_weights = np.array([[0 for _ in range(self.layer_neurons)]  for _ in range(self.inputs)])\n",
    "        \n",
    "        \n",
    "class NeuralNetwork:\n",
    "    def __init__(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.layers = None\n",
    "\n",
    "    # This method is used to add another layer to neural network, you can use it multiple times to add many many layers\n",
    "    def add_layer(self, layer_neurons, input_bias=False):\n",
    "        if self.layers is None:  # If first layer\n",
    "            self.layers = []\n",
    "            self.layers.append( Layer(layer_neurons, inputs=self.inputs, input_bias=input_bias) )\n",
    "        else:\n",
    "            prev_layer_neurons = self.layers[-1].layer_neurons  # New layer's input would be from previous layer\n",
    "            self.layers.append( Layer(layer_neurons, inputs=prev_layer_neurons, input_bias=input_bias) )\n",
    "    \n",
    "    \n",
    "    # This method returns neuron valyes at a layer\n",
    "    def neuron_values_at_layer(self, layer_wanted, input_values):\n",
    "        input_values_copy = input_values.copy()\n",
    "        \n",
    "        if layer_wanted < 0:  # As I have not made a layer object of Input so if input layer's neurons are required it would not exist in layers\n",
    "            if self.layers[0].input_bias:\n",
    "                input_values_copy.append(1)\n",
    "            return np.array(input_values_copy)  \n",
    "        \n",
    "        for ind_layer, layer in enumerate(self.layers):\n",
    "            if layer.input_bias:\n",
    "                input_values_copy.append(1)\n",
    "                \n",
    "            neuron_values = np.matmul(input_values_copy, layer.weights)\n",
    "#             neuron_values = [self.sigmoid(value) for value in neuron_values]  # As this was creating issue and weights were behaving abnormally\n",
    "            input_values_copy = neuron_values\n",
    "            \n",
    "            if ind_layer == layer_wanted:\n",
    "                if layer.input_bias:\n",
    "                    neuron_values = list(neuron_values)\n",
    "                    neuron_values.append(1)\n",
    "                return np.array(neuron_values)\n",
    "    \n",
    "    # Calculate sigmoid\n",
    "    def sigmoid (self, value):\n",
    "        return 1 / (1 + np.exp(-value))\n",
    "    \n",
    "    # Forward propogate input value to current network with current weights\n",
    "    def forward_propogate(self, input_values):\n",
    "        input_values_copy = input_values.copy()\n",
    "        for layer in self.layers:\n",
    "            if layer.input_bias:\n",
    "                input_values_copy.append(1)\n",
    "\n",
    "            neuron_values = np.matmul(input_values_copy, layer.weights)  \n",
    "            neuron_values = [self.sigmoid(value) for value in neuron_values]\n",
    "            input_values_copy = neuron_values\n",
    "        output = self.sigmoid(neuron_values[0])\n",
    "#         output = neuron_values[0]\n",
    "        return output\n",
    "    \n",
    "    # Gradient descent algorithm\n",
    "    def gradient_descent(self, train_examples, labels, learning_rate, epochs):\n",
    "        # Step 1: Initialize each weight to small random value\n",
    "            # Already doing while initalizing layers\n",
    "        \n",
    "        # Step 2: Until Termination condition is met\n",
    "        for _ in range(epochs):\n",
    "            \n",
    "            # Step 3: Initialize delta_weights to 0 \n",
    "            for layer in self.layers:\n",
    "                layer.initialize_delta_weights()\n",
    "                \n",
    "            # Step 4: For each individual example in all training examples\n",
    "            for train_example in train_examples:\n",
    "                \n",
    "                # Step 5: Forward propogate and compute output\n",
    "                prediction = self.forward_propogate(train_example)\n",
    "                \n",
    "                # Step 6: Compute delta_weights\n",
    "                for ind, layer in enumerate(self.layers):\n",
    "                    error_gradient = (labels[ind]-prediction) * (prediction*(1-prediction)) * self.neuron_values_at_layer(ind-1, train_example)  # see equation in slides/video lecture for sigmoid\n",
    "                    layer.delta_weights = ( layer.delta_weights.T + (learning_rate * error_gradient) ).T\n",
    "                \n",
    "            # Step 7: Update weights using delta_weights\n",
    "            for layer in self.layers:\n",
    "#                 print(layer.weights, '-----Updating---')\n",
    "                layer.weights = layer.weights + layer.delta_weights\n",
    "#                 print(layer.weights, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(inputs=2)\n",
    "model.add_layer(layer_neurons=2)\n",
    "model.add_layer(layer_neurons=1)\n",
    "\n",
    "x = [ [0,0], [0,1], [1,0], [1,1] ]\n",
    "y = [0, 1, 1, 0]\n",
    "model.gradient_descent(x, y, 0.01, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0]  ->  0.500000000    actual: 0 predicted: 0\n",
      "[0, 1]  ->  0.620745754    actual: 1 predicted: 1\n",
      "[1, 0]  ->  0.621908601    actual: 1 predicted: 1\n",
      "[1, 1]  ->  0.622459153    actual: 0 predicted: 1\n"
     ]
    }
   ],
   "source": [
    "for ind, inp in enumerate(x):\n",
    "    out = model.forward_propogate(inp)\n",
    "    print(inp, ' -> ', '{:.9f}'.format(out), '  ', 'actual:', y[ind], 'predicted:', 1 if out>0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Layer:\n",
    "#     def __init__(self, layer_neurons, activation, inputs=False, input_bias=False):\n",
    "#         self.layer_neurons = layer_neurons\n",
    "#         self.input_bias = input_bias\n",
    "#         self.inputs = inputs + input_bias\n",
    "#         self.activation = activation\n",
    "#         self.delta_weights = None\n",
    "#         if inputs:\n",
    "#             self.weights = np.array([[rand.uniform(0,1) for _ in range(self.layer_neurons)]  for _ in range(self.inputs)])\n",
    "            \n",
    "#     def initialize_delta_weights(self):\n",
    "#         self.delta_weights = np.array([[0 for _ in range(self.layer_neurons)]  for _ in range(self.inputs)])\n",
    "      \n",
    "#     def sigmoid(self, value):\n",
    "#         return 1 / (1 + np.exp(-value))\n",
    "    \n",
    "#     def relu(self, value):\n",
    "#         return max(0, value)\n",
    "    \n",
    "#     def sigmoid_derivative(self, value):\n",
    "#         return value * (1-value)\n",
    "    \n",
    "#     def relu_derivative(self, value):\n",
    "#         if value <= 0:\n",
    "#             return 0\n",
    "#         else:\n",
    "#             return 1\n",
    "    \n",
    "#     def apply_activation(self, value):\n",
    "#         if self.activation == 'sigmoid':\n",
    "#             return self.sigmoid(value)\n",
    "        \n",
    "#         elif self.activation == 'relu':\n",
    "#             return self.relu(value)\n",
    "    \n",
    "#     def derivative(self, value):\n",
    "#         if self.activation == 'sigmoid':\n",
    "#             return self.sigmoid_derivative(value)\n",
    "        \n",
    "#         elif self.activation == 'relu':\n",
    "#             return self.relu_derivative(value)\n",
    "        \n",
    "# class NeuralNetwork:\n",
    "#     def __init__(self, inputs):\n",
    "#         self.inputs = inputs\n",
    "#         self.layers = None\n",
    "\n",
    "#     def add_layer(self, layer_neurons, activation, input_bias=False):\n",
    "#         if self.layers is None:\n",
    "#             self.layers = []\n",
    "#             self.layers.append( Layer(layer_neurons, inputs=self.inputs, activation=activation, input_bias=input_bias) )\n",
    "#         else:\n",
    "#             prev_layer_neurons = self.layers[-1].layer_neurons\n",
    "#             self.layers.append( Layer(layer_neurons, inputs=prev_layer_neurons, activation=activation, input_bias=input_bias) )\n",
    "    \n",
    "#     def neuron_values_at_layer(self, layer_wanted, input_values):\n",
    "#         input_values_copy = input_values.copy()\n",
    "        \n",
    "#         if layer_wanted < 0:\n",
    "#             if self.layers[0].input_bias:\n",
    "#                 input_values_copy.append(1)\n",
    "#             return np.array(input_values_copy)\n",
    "        \n",
    "#         for ind_layer, layer in enumerate(self.layers):\n",
    "#             if layer.input_bias:\n",
    "#                 input_values_copy.append(1)\n",
    "                \n",
    "#             neuron_values = np.matmul(input_values_copy, layer.weights)\n",
    "#             neuron_values = [layer.apply_activation(value) for value in neuron_values]\n",
    "#             input_values_copy = neuron_values\n",
    "            \n",
    "#             if ind_layer == layer_wanted:\n",
    "#                 if layer.input_bias:\n",
    "#                     neuron_values.append(1)\n",
    "#                 return np.array(neuron_values)\n",
    "                    \n",
    "#     def forward_propogate(self, input_values):\n",
    "#         input_values_copy = input_values.copy()\n",
    "#         for layer in self.layers:\n",
    "#             if layer.input_bias:\n",
    "#                 input_values_copy.append(1)\n",
    "# #             print('\\n', input_values_copy, ' X ', layer.weights, '\\n')                \n",
    "#             neuron_values = np.matmul(input_values_copy, layer.weights)\n",
    "#             neuron_values = [layer.apply_activation(value) for value in neuron_values]\n",
    "#             input_values_copy = neuron_values\n",
    "#         output = neuron_values[0]\n",
    "# #         return 1 if output[0] >= 0.5 else 0\n",
    "#         return output\n",
    "    \n",
    "#     def gradient_descent(self, train_examples, labels, learning_rate, epochs):\n",
    "#         # Step 1: Initialize each weight to small random value\n",
    "#             # Already doing while initalizing layers\n",
    "#         # Step 2: Until Termination condition is met\n",
    "#         for _ in range(epochs):\n",
    "#             # Step 3: Initialize delta_weights to 0 \n",
    "#             for layer in self.layers:\n",
    "#                 layer.initialize_delta_weights()\n",
    "                \n",
    "#             # Step 4: For each individual example in all training examples\n",
    "#             for train_example in train_examples:\n",
    "#                 # Step 5: Forward propogate and compute output\n",
    "#                 prediction = self.forward_propogate(train_example)\n",
    "#                 # Step 6: Compute delta_weights\n",
    "#                 for ind, layer in enumerate(self.layers):\n",
    "#                     error_gradient = (labels[ind]-prediction) * layer.derivative(prediction) * self.neuron_values_at_layer(ind-1, train_example)\n",
    "#                     layer.delta_weights = ( layer.delta_weights.T + (learning_rate * error_gradient) ).T\n",
    "                \n",
    "#             # Step 7: Update weights using delta_weights\n",
    "#             for layer in self.layers:\n",
    "#                 print(layer.weights, '-----Updating---')\n",
    "#                 layer.weights = layer.weights + layer.delta_weights\n",
    "#                 print(layer.weights, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Layer:\n",
    "#     def __init__(self, layer_neurons, inputs=False, weights_bias=False):\n",
    "#         self.layer_neurons = layer_neurons\n",
    "#         self.weights_bias = weights_bias\n",
    "#         if weights_bias:\n",
    "#             self.weights_bias = np.array([rand.uniform(0,1) for _ in self.layer_neurons])\n",
    "#         self.inputs = inputs + input_bias\n",
    "#         self.delta_weights = None\n",
    "#         self.delta_weights_bias = None\n",
    "#         if inputs:\n",
    "#             self.weights = np.array([[rand.uniform(0,1) for _ in range(self.layer_neurons)]  for _ in range(self.inputs)])\n",
    "            \n",
    "#     def initialize_delta_weights(self):\n",
    "#         self.delta_weights = np.array([[0 for _ in range(self.layer_neurons)]  for _ in range(self.inputs)])\n",
    "#         if self.weights_bias:\n",
    "#             self.delta_weights_bias = np.array([0 for _ in range(self.layer_neurons)])\n",
    "        \n",
    "# class NeuralNetwork:\n",
    "#     def __init__(self, inputs):\n",
    "#         self.inputs = inputs\n",
    "#         self.layers = None\n",
    "\n",
    "#     def add_layer(self, layer_neurons, weights_bias=False):\n",
    "#         if self.layers is None:\n",
    "#             self.layers = []\n",
    "#             self.layers.append( Layer(layer_neurons, inputs=self.inputs, weights_bias=weights_bias) )\n",
    "#         else:\n",
    "#             prev_layer_neurons = self.layers[-1].layer_neurons\n",
    "#             self.layers.append( Layer(layer_neurons, inputs=prev_layer_neurons, weights_bias=weights_bias) )\n",
    "    \n",
    "#     def neuron_values_at_layer(self, layer_wanted, input_values):\n",
    "#         input_values_copy = input_values.copy()\n",
    "        \n",
    "#         if layer_wanted < 0:\n",
    "#             if self.layers[0].input_bias:\n",
    "#                 input_values_copy.append(1)\n",
    "#             return np.array(input_values_copy)\n",
    "        \n",
    "#         for ind_layer, layer in enumerate(self.layers):\n",
    "#             if layer.input_bias:\n",
    "#                 input_values_copy.append(1)\n",
    "                \n",
    "#             neuron_values = np.matmul(input_values_copy, layer.weights)\n",
    "#             neuron_values = [self.sigmoid(value) for value in neuron_values]\n",
    "#             input_values_copy = neuron_values\n",
    "            \n",
    "#             if ind_layer == layer_wanted:\n",
    "#                 if layer.input_bias:\n",
    "#                     neuron_values.append(1)\n",
    "#                 return np.array(neuron_values)\n",
    "    \n",
    "#     def sigmoid (self, value):\n",
    "#         return 1 / (1 + np.exp(-value))\n",
    "    \n",
    "#     def calculate_half_squared_error(self, actual, predicted):\n",
    "#         error = 0\n",
    "#         for i in range(actual):\n",
    "#             error = error +  (( abs(actual[i]-predicted[i]) )**2) * (1/2)\n",
    "#         return error\n",
    "            \n",
    "#     def forward_propogate(self, input_values):\n",
    "#         input_values_copy = input_values.copy()\n",
    "#         for layer in self.layers:\n",
    "#             if layer.input_bias:\n",
    "#                 input_values_copy.append(1)\n",
    "# #             print('\\n', input_values_copy, ' X ', layer.weights, '\\n')                \n",
    "#             neuron_values = np.matmul(input_values_copy, layer.weights)\n",
    "#             neuron_values = [self.sigmoid(value) for value in neuron_values]\n",
    "#             input_values_copy = neuron_values\n",
    "# #         output = self.sigmoid(neuron_values[0])\n",
    "#         output = neuron_values[0]\n",
    "#         return output\n",
    "    \n",
    "#     def gradient_descent(self, train_examples, labels, learning_rate, epochs):\n",
    "#         # Step 1: Initialize each weight to small random value\n",
    "#             # Already doing while initalizing layers\n",
    "#         # Step 2: Until Termination condition is met\n",
    "#         for _ in range(epochs):\n",
    "#             # Step 3: Initialize delta_weights to 0 \n",
    "#             for layer in self.layers:\n",
    "#                 layer.initialize_delta_weights()\n",
    "                \n",
    "#             # Step 4: For each individual example in all training examples\n",
    "#             for train_example in train_examples:\n",
    "#                 # Step 5: Forward propogate and compute output\n",
    "#                 prediction = self.forward_propogate(train_example)\n",
    "#                 # Step 6: Compute delta_weights\n",
    "#                 for ind, layer in enumerate(self.layers):\n",
    "#                     error_gradient = (labels[ind]-prediction) * (prediction*(1-prediction)) * self.neuron_values_at_layer(ind-1, train_example)\n",
    "#                     layer.delta_weights = ( layer.delta_weights.T + (learning_rate * error_gradient) ).T\n",
    "#                     layer.delta_weights\n",
    "                \n",
    "#             # Step 7: Update weights using delta_weights\n",
    "#             for layer in self.layers:\n",
    "#                 print(layer.weights, '-----Updating---')\n",
    "#                 layer.weights = layer.weights + layer.delta_weights\n",
    "#                 print(layer.weights, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
