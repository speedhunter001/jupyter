{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Time Object Detection with Image Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Matching + Homography to find Objects.\n",
    "\n",
    "We have learned how to match descriptors of two images and then check the number of matches to determine if the object is present in the image or not. Similarly we can take this one step further and even get the coordinates of the object in another image Via Homography. Since in Homography you estimate the transformation of an image (in this case an object) by working with the transformed points. In our *Geometric Tranformations Notebook* we have already learned how to get the 3x3 homography matrix using the function **cv2.getPerspectiveTransform()** and then used to warp the image with **cv2.warpPerspective()** but this time we will be using a more robust approach primarily because last time our points were selected by us but this time they will approximated by feature detection algorithm, so there will possibly be outliers.\n",
    "\n",
    "You're already familiar with these outliers as you have seen there are almost always mis-matches, and these can effect the results. So in our implementation we will be using **cv2.findHomography()** to find the homography matrix and then **cv2.perspectiveTransform()** to find the object.\n",
    "\n",
    "### Finding Homography\n",
    "\n",
    "\n",
    "[```\tMatrix, mask = cv2.findHomography(srcPoints, dstPoints[, method[, ransacReprojThreshold[, mask[, maxIters[, confidence]]]]]) ```](https://docs.opencv.org/master/d9/d0c/group__calib3d.html#ga4abc2ece9fab9398f2e560d53c8c9780)\n",
    "\n",
    "**Params:**\n",
    "\n",
    "- **srcPoints**\tCoordinates of the points in the original plane, a matrix of the type CV_32FC2 or vector<Point2f> .\n",
    "- **dstPoints**\tCoordinates of the points in the target plane, a matrix of the type CV_32FC2 or a vector<Point2f> .\n",
    "- **method**\tMethod used to compute a homography matrix. The following methods are possible:\n",
    "    1.  **`0`** - a regular method using all the points, i.e., the least squares method\n",
    "    2. **`RANSAC`** - RANSAC-based robust method\n",
    "    3. **`LMEDS`** - Least-Median robust method\n",
    "    4. **`RHO`** - PROSAC-based robust method\n",
    "\n",
    "\n",
    "- **ransacReprojThreshold**\tMaximum allowed reprojection error to treat a point pair as an inlier (used in the RANSAC and RHO methods only). That is, if: <br>\n",
    "          `  ∥dstPointsi−convertPointsHomogeneous(H∗srcPointsi)∥2>ransacReprojThreshold`\n",
    "then the point i is considered as an outlier. If srcPoints and dstPoints are measured in pixels, it usually makes sense to set this parameter somewhere in the range of 1 to 10.\n",
    "- **mask**\tOptional output mask set by a robust method ( RANSAC or LMEDS ). Note that the input mask values are ignored.\n",
    "- **maxIters**\tThe maximum number of RANSAC iterations.\n",
    "- **confidence**\tConfidence level, between 0 and 1.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Perspective Transformation\n",
    "\n",
    "```dst = cv2.perspectiveTransform( src, m[, dst] )```\n",
    "\n",
    "**Params:**\n",
    "\n",
    "\n",
    "- **`src`**\tinput two-channel or three-channel floating-point array; each element is a 2D/3D vector to be transformed.\n",
    "- **`dst`**\toutput array of the same size and type as src.\n",
    "- **`m`**\t3x3 or 4x4 floating-point transformation matrix.\n",
    "\n",
    "\n",
    "### Note:\n",
    "Again both methods **getPerspectiveTransform()** and **cv2.findHomography()** will find the perspective transformation between two sets of points and here is the difference between the two.\n",
    "\n",
    "**getPerspectiveTransform()**  is useful in many situations where you only have 4 points, and you are sure they are the correct ones (e.g you yourself marked those points). The **findHomography()** is usually used when you're trying to detect the points with some automatic method (like feature detection), in this scenario you will have lots of points with low confidence, and so inside the **findHomography()** you will use an algorithm called **RANSAC** or **LEAST_MEDIAN**, these algorithms will separate \n",
    "inlier points with the outlier points.\n",
    "\n",
    "So good matches which provide correct estimation of object are called inliers and remaining are called outliers. **cv2.findHomography()** returns a mask which specifies the inlier and outlier points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Matching + Detection With ORB\n",
    "First lets detect features and match descriptors to get some good matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread('media/M4/dino.png',1)  # queryImage\n",
    "img2 = cv2.imread('media/M4/toys1.png',1) # trainImage\n",
    "\n",
    "# Initialize ORB detector\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# Find the keypoints and descriptors with ORB\n",
    "kp1, des1 = orb.detectAndCompute(img1,None)\n",
    "kp2, des2 = orb.detectAndCompute(img2,None)\n",
    "\n",
    "index_params= dict(algorithm = 6, table_number = 6, key_size = 12, multi_probe_level = 1)\n",
    "\n",
    "search_params = dict(checks=50)   \n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.70*n.distance:\n",
    "        good.append([m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min number of matches to call it a match.\n",
    "MIN_MATCH_COUNT = 10\n",
    "\n",
    "if len(good)>MIN_MATCH_COUNT: \n",
    "    \n",
    "    # We are grabbing all the good matched points from both the images and formating them in a list\n",
    "    # the details of below line are at the end\n",
    "    src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    \n",
    "    dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    \n",
    "    # A 3x3 transformation matrix is obtained\n",
    "    M , _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "       \n",
    "    # Extracting the height and width of the Query Image.    \n",
    "    h,w = img1.shape\n",
    "    \n",
    "    pts = np.float32([ [0,0],[0,h],[w,h],[w,0] ]).reshape(-1,1,2)  #now we can draw the polygon according to the size of\n",
    "    #our query image\n",
    "    \n",
    "    # Takes in the initial points and transformation matrix and returns the perspective transform\n",
    "    # If your object is a rectangle then these are just your usual 4 boundary points.\n",
    "    dst = cv2.perspectiveTransform(pts,M)\n",
    "    \n",
    "    # Drawing a polygon on the detected Object. We are not drawing a rectangle since your object may be rotated at an angle.\n",
    "    final_image = cv2.polylines(imgcolored, [np.int32(dst)], True, (255,0,0), 3, cv2.LINE_AA)\n",
    "  \n",
    "else:\n",
    "    print(\"Not enough matches are found \" + len(good))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min number of matches to call it a match.\n",
    "MIN_MATCH_COUNT = 10\n",
    "\n",
    "img1 = cv2.imread('media/M6/dino.png',0)          # queryImage\n",
    "img2 = cv2.imread('media/M6/toys1.png',0) # trainImage\n",
    "imgcolored = cv2.imread('media/M6/toys1.png') #just for drawing on colored image\n",
    "\n",
    "# Initiate SIFT detector\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "\n",
    "search_params = dict(checks = 50)\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "matches = flann.knnMatch(des1,des2,k=2)\n",
    "# store all the good matches as per Lowe's ratio test.\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        good.append(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set a condition that atleast 10 matches (defined by MIN_MATCH_COUNT) are to be there to find the object. Otherwise simply show a message saying not enough matches are present.\n",
    "\n",
    "If enough matches are found, we extract the locations of matched keypoints in both the images. They are passed to find the perpective transformation. Once we get this 3x3 transformation matrix, we use it to transform the corners of queryImage to corresponding points in trainImage. Then we draw it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(good)>MIN_MATCH_COUNT: \n",
    "    \n",
    "    #we are grabbing all the good matched points from both the images and formating them in a list\n",
    "    # the details of below line are at the end\n",
    "    src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    \n",
    "    dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    \n",
    "    # the transformation matrix is obtained\n",
    "    M, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "    # this is just a 3 by 3 matrix\n",
    "    \n",
    "        \n",
    "    h,w = img1.shape\n",
    "    \n",
    "    pts = np.float32([ [0,0],[0,h],[w,h],[w,0] ]).reshape(-1,1,2)  #now we can draw the polygon according to the size of\n",
    "    #our query image\n",
    "    \n",
    "    #gives the perspective transform these are just 4 points for our polygon to be drawn\n",
    "    dst = cv2.perspectiveTransform(pts,M)\n",
    "    \n",
    "\n",
    "    img2 = cv2.polylines(imgcolored ,[np.int32(dst)],True,(255,0,0),3, cv2.LINE_AA)\n",
    "  \n",
    "else:\n",
    "    print(\"Not enough matches are found \" + len(good))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img3 = cv2.drawMatches(img1,kp1,img2,kp2,good,None,flags=2)\n",
    "cv2.imshow('pic',img2)\n",
    "\n",
    "cv2.waitKey(0) \n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.int32(dst)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m) # its a dmatch object\n",
    "print(m.queryIdx)  # its the index of the point\n",
    "print(kp1[m.queryIdx]) #its a class object\n",
    "print(kp1[m.queryIdx].pt)  # the actual point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.float32([ kp1[m.queryIdx].pt for m in good ]).shape)\n",
    "print(np.float32([ kp1[m.queryIdx].pt for m in good ]).ndim)\n",
    "\n",
    "print(src_pts.shape)\n",
    "print(src_pts.ndim)#visually you can see that after reshaping the only difference is that there are two extra brackets after \n",
    "# reshaping at the beggining and the end ,  src_pts.ndim is same as img2.ndim \n",
    "print(src_pts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Almost Real Time Object Detection And Tracking with Sift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "img = cv2.imread(\"media/M6/thecroper.jpg\", 0) # queryiamge\n",
    "\n",
    " \n",
    "cap = cv2.VideoCapture(1)\n",
    " \n",
    "# Features\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "#surf = cv2.xfeatures2d.SURF_create(1000)\n",
    "kp_image, desc_image = sift.detectAndCompute(img, None)\n",
    " \n",
    "# Feature matching\n",
    "index_params = dict(algorithm=0, trees=5)\n",
    "search_params = dict()\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "    grayframe = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # trainimage\n",
    " \n",
    "    kp_grayframe, desc_grayframe = sift.detectAndCompute(grayframe, None)\n",
    "    matches = flann.knnMatch(desc_image, desc_grayframe, k=2)\n",
    " \n",
    "    good_points = []\n",
    "    for m, n in matches:\n",
    "        if m.distance <0.6* n.distance:\n",
    "            good_points.append(m) \n",
    " \n",
    "    # Homography\n",
    "    if len(good_points) > 10:\n",
    "        query_pts = np.float32([kp_image[m.queryIdx].pt for m in good_points]).reshape(-1, 1, 2)\n",
    "        train_pts = np.float32([kp_grayframe[m.trainIdx].pt for m in good_points]).reshape(-1, 1, 2)\n",
    " \n",
    "        matrix, mask = cv2.findHomography(query_pts, train_pts, cv2.RANSAC, 5.0)\n",
    "        #matches_mask = mask.ravel().tolist() #not requried if  ure not gonna draw matches\n",
    " \n",
    "        # Perspective transform\n",
    "        h, w = img.shape\n",
    "        pts = np.float32([[0, 0], [0, h], [w, h], [w, 0]]).reshape(-1, 1, 2)\n",
    "        dst = cv2.perspectiveTransform(pts, matrix)\n",
    " \n",
    "        homography = cv2.polylines(frame, [np.int32(dst)], True, (0, 255, 0), 3)\n",
    " \n",
    "        cv2.imshow(\"Homography\", homography)\n",
    "       # print(len(good_points))\n",
    "    else:\n",
    "        cv2.imshow(\"Homography\", frame)\n",
    "       # print(len(good_points))\n",
    " \n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    " \n",
    " \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Time Object Detection And Tracking with Orb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "img = cv.imread(\"media/M6/thecroper.jpg\", 0)          # queryImage\n",
    "\n",
    "cap = cv.VideoCapture(1)\n",
    " \n",
    "# Features\n",
    "\n",
    " \n",
    "# Initiate ORB detector\n",
    "orb = cv.ORB_create(4000)\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp_image, desc_image = orb.detectAndCompute(img, None)\n",
    "\n",
    "# FLANN parameters\n",
    "FLANN_INDEX_LSH = 6\n",
    "index_params= dict(algorithm = FLANN_INDEX_LSH,\n",
    "                   table_number = 6,  \n",
    "                   key_size = 12,   \n",
    "                   multi_probe_level = 1)\n",
    "search_params = dict(checks=100)   # or pass empty dictionary like this: dict() or {}\n",
    "flann = cv.FlannBasedMatcher(index_params,search_params)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        grayframe = cv.cvtColor(frame, cv.COLOR_BGR2GRAY) # trainimage\n",
    " \n",
    "        kp_grayframe, desc_grayframe = orb.detectAndCompute(grayframe, None)\n",
    "        matches = flann.knnMatch(desc_image, desc_grayframe, k=2)\n",
    "        try:\n",
    "            good_points = []\n",
    "            for m, n in matches:\n",
    "                if m.distance <0.6* n.distance:\n",
    "                    good_points.append(m) \n",
    "\n",
    "            # Homography\n",
    "            if len(good_points) > 8:\n",
    "                query_pts = np.float32([kp_image[m.queryIdx].pt for m in good_points]).reshape(-1, 1, 2)\n",
    "                train_pts = np.float32([kp_grayframe[m.trainIdx].pt for m in good_points]).reshape(-1, 1, 2)\n",
    "\n",
    "                matrix, mask = cv.findHomography(query_pts, train_pts, cv.RANSAC, 5.0)\n",
    "                matches_mask = mask.ravel().tolist()\n",
    "\n",
    "                # Perspective transform\n",
    "                h, w = img.shape\n",
    "                pts = np.float32([[0, 0], [0, h], [w, h], [w, 0]]).reshape(-1, 1, 2)\n",
    "                dst = cv.perspectiveTransform(pts, matrix)\n",
    "\n",
    "                homography = cv.polylines(frame, [np.int32(dst)], True, (0, 255, 0), 3)\n",
    "\n",
    "                cv.imshow(\"Homography\", homography)\n",
    "                #print(len(good_points))\n",
    "            else:\n",
    "                cv.imshow(\"Homography\", frame)\n",
    "                #print(len(good_points))\n",
    "        except:\n",
    "            cv.imshow(\"Homography\", frame)\n",
    "        \n",
    "    key = cv.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    " \n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine SIFT , Orb code in a single method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
